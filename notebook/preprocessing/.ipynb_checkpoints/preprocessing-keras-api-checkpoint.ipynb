{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import autoreload\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Module Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.preprocessing_helper import PreprocessingHelper\n",
    "from preprocessing_helper import PreProcessingHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_helper = PreProcessingHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"../../data/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
    "test_raw =pd.read_csv(\"../../data/testData.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "1. Define processing parameters such as the size of vocabulary, max length of document, etc\n",
    "2. Clean up raw training data, construct tokenizer on it\n",
    "3. Convert cleaned training data to word ids based on tokenizer\n",
    "4. Split training set in to training and validation set\n",
    "5. Clean up raw testing data\n",
    "6. Convert cleaned testing data to word ids based on tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "max_len = 200 # max length of a document\n",
    "save_tokenizer = True\n",
    "tokenizer_path = \"../../data/tokenizer.pickle\"\n",
    "val_size = 0.001 # the ratio of validation set and training set\n",
    "random_state = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_list = train_raw[\"review\"].to_list()\n",
    "train_cleaned = prep_helper.clean_docs(train_raw_list)\n",
    "tokenizer = prep_helper.setup_tokenizer(train_cleaned, vocab_size, save_tokenizer, tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prep_helper.clean_corpus_to_ids(train_cleaned, tokenizer, max_len)\n",
    "y_train = np.array(train_raw[\"sentiment\"].to_list(), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.001, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_list = test_raw[\"review\"].to_list()\n",
    "test_cleaned = prep_helper.clean_docs(test_raw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prep_helper.clean_corpus_to_ids(test_cleaned, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed data dumped into pickle file!\n"
     ]
    }
   ],
   "source": [
    "prep_helper.save_train_val_test([X_train, y_train, X_val, y_val, X_test], \"../../data/preprocessed_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
